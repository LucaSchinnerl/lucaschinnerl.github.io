<!DOCTYPE HTML>
<!--
	Landed by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Portfolio</title>
		
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<h1><a href="#" id="logo"></a></h1>
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

		<!-- Header -->
		<header id="header">
			<nav id="nav">
				<ul>
					<li><a href="index.html">Home</a></li>
					<li><a href="datascience.html">Portfolio</a></li>
					<!-- <li><a href="Physics.html">Physics</a></li>
					<li><a href="Software.html">Software</a></li> -->


				</ul>
			</nav>
		</header>

			<!-- Main -->
				<div id="main" class="wrapper style1">
					<div class="container">
						<header class="major">
							<h2>Data Science Portfolio</h2>
							<p>This page summarises some notable data science and data analysis projects I have completed over the years.</p>
						</header>

						<!-- Content -->
							<section id="content">




								<a href="#" class="image fit"><img src="images/pic07.jpg" alt="" /></a>
								
								<h2>Selective Background Monte Carlo Simulation at Belle II</h2>
								<p>
									<h4> History </h4>
									In high energy physics (HEP), physicists work with particle accelerators to analyze very specific processes that are happening when 
									particles interact with each other. The goal here is to find processes that with our current models
									we do not expected to occur. This could sound counterintuitive at first, why do we want to disprove our current theories you might ask?
									It is quite simple, we know that our current best theory of the universe, the standard model, is not the whole story. 
									In other words, we know there is something wrong with it, but we do not know what exactly yet (hopefully). 
									To find out what is wrong, we need data. A LOT of data.
									While gathering data for groundbreaking discoveries in the past was experimentally relatively easy, the story today is much more complicated.
									As an example, the experiment that led to the 1980 Nobel Prize in Physics and disproved a what was thought to be a fundamental symmetry 
									in the universe,
									was performed in just a few days. The two scientists behind it actually wanted to measure something completely different, but
									kind of stumbled upon their revolutionary discoveries. Meanwhile the 2013 Nobel Prize in Physics, the discovery of the Higgs Boson
									and the last Nobel Prize in HEP, was decades in the making. Thousands of scientists collaborated to slowly improve measurements
									and increase the statistical significance that this particle actually exists. For this a massive amount of data was collected.
									Measurements today require even more resources, time and most importantly data.
								</p>
								<p>
									<h4>The Belle II Experiment</h4>
									<span class="image right"><img src="images/Overview-of-the-Belle-II-detector.jpg" alt="" /></span>
									Belle II is a detector at the SuperKEKB particle accelerate located just outside of Tokyo. 
									On the right you can see this massive and beautiful machine.
									The goal of Belle II is to measure extremely rare processes, to find disagreements with the standard model.
									For this over its lifetime, this experiment will obtain an integrated luminosity of 50 inverse Attobarn worth of data.
									This is physics lingo for a massive amount of data (in the exabyte range).
									Performing any type of measurements on this data requires the modeling of the experiment as best as possible to 
									first prepare each analysis. 
									In the case of the Belle II experiment this is done in the form of Monte Carlo (MC) simulations. 
									This allows fine tuning physics analyses to be able to filter the signal, 
									so the physics process being searched for, from everything else, the background. 
									Since a large portion of the measurements will be focused on so-called rare processes, 
									a strong statistical knowledge of the backgrounds 
									is required to accurately distinguish the signal from the remaining background. 
									Simulating the required amount of data is not feasible using current or near future resources so we have to be creative. 
								<p>
								<p>
								
									<h4>Selective Background Monte Carlo Simulation</h4>
									<span class="image left"><img src="images/MC_time.png" alt="" /></span>
									On the left you can see a usual dataflow withing a MC simulation (disregard the NN filter for now). 
									We start of with an event generation stage (in green). Here we generate a random event that we except would occur in nature.
									We now want to know how the detector would measure this event. In other words how do we expect the detector output to look like if 
									our generated process were to be measured by it. This is done in the simulation and reconstruction phase.  
									In essence, we have to simulate every single component in 
									the massive detector you see above. This is computational extremely expensive and takes up more than 99% of time of the whole 
									MC simulation. Finally, in the last step, a skim is applied. This is a filter that uses the data that we have just simulated in 
									the step before. It is
									applied to the data to remove events that are not of interest for our physics analysis. 
									Depending on the analysis, less than 1% of events survive this step.
								</p>
								<p>
									Basically what we have done is we generated our events, painstakingly simulated all of them and then trivially threw 
									almost all of them away during skimming. This is horribly inefficient. 
									It would be nice if we already knew at the event generation stage, which events are going to be interesting for us and
									only then simulate exactly these events. This is what we mean by selective background Monte Carlo simulation.
									To do this we have injected a neural network between the event generation and simulation stage that 
									tries to predict which events are going to survive the skim from the information we have at the event generation.
								</p>

								<p>
									<h4>The Machine Learning Model</h4>
									<span class="image right"><img src="images/graph_structure.png" alt="" /></span>

									To built the perfect model, we need to know what information we have available and how it is structured.
									A decay process kind of looks like a tree. We start with a big and heavy particle which then splits into smaller and smaller ones 
									until we arrive at our end products. Particles are represented by nodes in a graph while the edges represent the decay channels.
									You can see an example of such a process on the right. Each node has some properties, like mass, charge, particle ID, energy etc.
									The best way to make use of this type of graph structured data is to use 
									a Graph Neural Network. Due to the fact that each graph provides a massive amount of data, it is reasonable to assume that we will 
									need a network with a huge amount of trainable parameters. The model that we have utilised is a Graph Attention Network with 
									Global Attention Pooling. In total there are over 100,000 trainable parameters in this network.

								</p>

								
								<p>
									<h4>Bias Mitigation</h4>
									<span class="image left"><img src="images/fun_MC.png" alt="" /></span>
									
									When applying the NN filter, we have to make sure that the NN does not introduce a bias in the resulting output. 
									In general, the NN can misclassify events in two ways. It can keep events that will later be discarded by the skim (false positives), or 									
									it can throw away events that would have passed the skim if it survived the NN filter (false negatives).
									
									The latter are actually very problematic.
									
									While false positives just reduce the performance, false negatives actually introduce a bias in our data.
								</p>
								<p>
									<span class="image right"><img src="images/bias_mbc.png" alt="" hight = 200/></span>
									On the right you can see an example of this. In simple terms what you see here is the reconstructed energy of the whole decay process.
									The higher we set the cut for discarding events with the NN output, the more biased our data gets.
									This is because the more selective we are with letting events thorough the NN filter, the more we introduce false negatives.
									This is unacceptable for an analysis and we have to mitigate this. Luckily some smart people have thought about some 
									extremely useful methods a few hundred years ago. The technique we have opted to use is importance sampling: 


								</p>
								<ul>
									<li> Instead of introducing a cut, we take the NN output as probability that we are keeping an event.</li>
									<li> We attach a weight to each event with \(w_i = \frac{1}{p_{NN}}\).</li>
									<li> With importance sampling there is no bias by construction.</li>

								</ul>
	
								With this relatively easy method, we were able to mitigate the complete bias. 
								However, due to the fact that we are working with weighted events now (some events are now more important than others), 
								we have to define a new measure on how many events we have simulated.
								Just using the total amount of events is not sufficient.
								In importance sampling, this measure is called the <b>Effective Sample Size</b>:
								$$N_{eff} = \frac{(\sum_i w_i)^2}{\sum_i w_i ^ 2}$$

								This equation relates the information (or statistics) we have with unweighted events (just the number of events that have passed the skim) 
								to the mix of weighted events we have after importance sampling.

								Finally, what we want to optimse with the NN filter is the speedup, how much faster we can simulate the same effective sample size, rather
								than the accuracy of the model. While the accuracy is correlated with the speedup, it is not the most efficient way to train the network.
								Instead we have opted to use a custom loss function:

								$$R_0 = \frac{t_{\text{no_filter}}}{t_{\text{filter}}}$$
								
								This is the ratio of the time it takes to simulate the same effective sample size with the NN filter and without it.
								If it gets lower, the performance is larger.

								<p>
									<h4>Analysis</h4>
								
								<span class="image right"><img src="images/train_loss_nROE.png" alt="" hight = 200/></span>

								To see how selective background MC simulation performs in practice, I have analysed the NN filter on a specific decay process:
								<h2>
								$$
								B \rightarrow K \nu \nu
								$$
								</h2>
								This process involves a B meson decaying into a K0 and two neutrinos and can only be mediated through flavor-changing neutral-current transitions.
								The process is described in detail in <a href = https://arxiv.org/pdf/1303.7465.pdf>this paper </a>. All this complicated terminology just means 
								that this process is ultra rare, highly sensitive to new physics (and therefore a good contender for groundbreaking discoveries) and a lot 
								of data is required for the analysis. A good candidate for our NN filter.
								The cuts I have applied in the skim to produce the training data are:
								<ul>
									<li> Increasing the cut on the reconstructed energy of the mother particle (the B mesons)) and 
										the reconstruction probabilities of the particles. This is motivated by increasing
										efficiencies on the reconstructed origin particle (the \(B_{\text{tag}}\)).</li>
									<li> Require that a the whole decay chain could be reconstructed.
										In technical terms, was there a \(\Upsilon(4S)\) with a \(B \rightarrow K^{(*)}\nu \bar{\nu}\)?</li>
									<li> 
										Ensures that all particles from the primary physics event (\(B \rightarrow K \nu \nu\)) were used in the reconstruction.
										In other words, require that there are no chared tracks in the rest of event when  we reconstructed the \(\Upsilon(4S)\).</li>
								</ul>
								
								On the right you can see the loss evolution when training the NN. Remember the loss here represents the inverse speedup, if it is smaller than 
								1 the NN is performing better than a brute force approach. You can also see than when I allow fewer chared tracks in the rest of event, 
								the performance increases. The best performance is achieved when no tracks are allowed, showing that my previous applied cuts are indeed sensible.
								All in all the best speedup I was able to achieve was just less than 5. This means with the NN filter we can simulate events 5 times as fast than without.

								
								</p>

								<p>	
									<h4>Conclusion</h4>
									In this short blog post I was not able to go into all the technical details and had to omit a lot of very important information.
									Once I finish writing my master thesis I will include this, so if you are interested in the technical details, please check it out then.
									For now you can go through my a little outdated slides for a talk I gave at 
									the <a href = "https://www.dpg-verhandlungen.de/year/2022/conference/heidelberg/part/t/session/53/contribution/2">
										DPG conference in Heidelberg</a> (see blow for slides).
									However, I think this clearly demonstrates that smart background simulation has the potential to significantly improve simulation speeds.
									For the process \(B \rightarrow K^{(*)}\nu \bar{\nu}\), this has worked especially well, improving the 
									the rate of the speedup to almost 5.

								</p>
								
								<p>
									<ul class="actions">
									<li><a href="PDF/practial_phase_talk.pdf" class="button">DPG Talk</a></li>
									</ul>
	
							</p>






							<hr />

								
								<h2>Estimating the Betweenness Centrality of Nodes with Network Embedding</h2>
								<p>
									In graph theory, a key area is to estimate the importance of specific nodes in a network. This is usually done with a centrality measure. 
									There are many different types of these centrality measures and they can be classified under the following categories (among others):
	
									<ul>
										<li> Degree Centrality;</li>
										<li> Eigenvector Centrality</li>
										<li> Closeness Centrality;</li>
										<li> Shortest Path Centrality;</li>
										<li> Group Centrality.</li>
									</ul>
									<span class="image left"><img src="images/graphempty.png" alt="" /></span>
									However, a widely accepted method to quantify the importance of nodes is the betweenness centrality. 
									Suppose we have a graph just like the one on the left. Which one do you think is the most important node? 
									Most people would probably suggest node 4 as it connects the whole graph together. How can this be quantified? 
									The most effective way to do this is to use exactly this betweeness centrality.
									The method is based on a shortest paths 
									approach. For every pair of vertices in a connected graph, there exists at least one shortest path between the vertices such that either the number of 
									edges that the path passes through (for unweighted graphs) or the sum of the weights of the edges (for weighted graphs) is minimised. The betweenness 
									centrality for each vertex is proportional to the number of these shortest paths that pass through the vertex. 
									Going back to the example on the left, if you count all the shortest paths that run through node 4, we get 15. In comparison node 5 only has 6 shortest paths running through it. 
									In other words, the betweenness centrality is very effective in identifying that node 4 is by far the most important node in the graph. 
									This all sounds great, but the problem is that calculating the exact betweenness centrality is extremely computationally expensive. 
									A network with tens of thousands of nodes and millions of connections, it just is not feasible nor necessary to calculate this exactly.
	
									My research discusses a new algorithm which can be used to approximate the betweenness centrality for undirected and unweighted graphs. 
									The discussion includes how the algorithm is able to approximate this centrality for very large networks, where a direct calculation is highly complex and takes a very long time.
									This approach is based on network embedding and borrows tools from natural language processing. The algorithm goes as follows:
								</p>
								<p>
	
								<ol>  
									<li>Randomly choose two different nodes on the network;</li>  
									<li>Perform multiple weighted random walks on the network for both of the selected nodes, as seen on the figure to the right;</li>  
									<li>Order all visited nodes by their frequency of appearing from all of these random walks for both starting nodes;</li>  
									<li>Move embedded nodes closer to each other in the embedding space which appeared with a similar frequency.</li>  
								   </ol>  
								
									</p>
									<p>
										<span class="image right"><img src="images/randomwalkexp.png" alt="" /></span>
										Extensive testing of this method has shown that the approximation is only accurate for large and complex real world networks. 
										For small uncomplicated graphs, there just is not enough information present for the algorithm to perform a good enough approximation.
										In addition, the algorithm has shown to not be able to effectively and accurately approximate the betweenness centrality for scale free synthetic random graphs.
										However, this is not a big problem, as for small networks with low mean degree, approximation does not make sense anyway, as the exact centrality can be computed very easily.
										Furthermore, applications to synthetic random graphs which in most cases do not represent any attributes from real world graphs is also not important. 
										Applications to very large and complicated real world networks is what is important, which the algorithm is able to handle.
	
	
	
								   
								</p>
	
								<p>
									<span class="image left"><img src="images/facebook_results.png" alt="" /></span>
									An application for this algorithm could for example be to estimate the most connected people on facebook. For this, 
									Facebook provides a very nice dataset. 
									It consists of 'circles' (or 'friends lists') from facebook users, from which a graph can be created. 
									The data was gathered from individual survey participants using the Facebook app. This network includes 4039 nodes and 88234 edges.					
									In the figure on the left you can see a plot of the nodes embedded into 2 dimensions. The numbers next to each dot represent the exact betweenness centrality.
									The algorithm has quite nicely isolated nodes with a high centrality. In this case it can clearly be seen that more "important" nodes are located further to the right. 
									All the "unimportant" nodes with a centrality close to 0 are clumped together in a big pile on the left.
									This means that on this real world data set, the algorithm has been able to identify the most central nodes present in the network.
									In addition, by choosing the right hyperparameters, the algorithm was a lot faster than the exact calculation.
									<p>
										All in all, it can be said that generally the algorithm is successful for huge and complex real world (scale free) graphs, while applications to small, simple and non scale free graphs is limited.
										Important nodes which act as hubs in the network are almost always accurately identified and isolated from unimportant nodes in the embedding space.
										
									</p>
									 
								
	</p>	
	
								<p>
									<ul class="actions">
									<li><a href="PDF/Estimating the Betweenness Centrality of Nodes with Network Embedding.pdf" class="button">Full paper</a></li>
									</ul>
	
							</p>

							<hr />
								<h2>Classification of the Ising Model</h2>
								<p>
									<span class="image left"><img src="images/spin0.png" alt="" /></span>
									<span class="image left"><img src="images/spin1.png" alt="" /></span>
									In statistical physics, scientists have been attempting to model physical systems for centuries. The Ising model, named after the physicist Ernst Ising, 
									is such a model for ferromagnetism (the mechanism of permanent magnets). A discrete variable that describes the magnetic dipole (the spin) describes the model. 
									The possible values of this variable are either spin up or spin down identified as +1 or -1.
									The variable is placed on a graph, in this case on a 2 dimensional lattice. On the left you can see two examples of this.
									Yellow represent a spin of +1, while purple represents -1. 
									The model is dynamic and each spin interacts with its direct neighbors.
									
									Neighboring spins that have the same value, have a lower energy than those that disagree. 
									This means in a system with only spin downs, energy in the system would be minimised.
									Similarly to an object falling of a table, every physical system tends to move to its lowest energy state, given enough time.

								</p>	
								<p>
				
									To simulate dynamics in the system, spins can randomly change with a probability that is directly proportional to the temperature of the system.
									Because of this, the system tends to the lowest energy but heat disturbs this tendency, thus creating the possibility of different structural phases. 
									This can again be seen by the two figures above. The left images shows a cold and nicely ordered state. On the right the system is hot and chaotic.
									The question now is, how efficiently can an AI differentiate these two states and how much information can be thrown away for an AI to still distinguish them?
									As it turns out very little information is required to efficiently classify a given dataset. 
									The following machine learning algorithms were implemented:
										<ul>
											<li>Classification with mostly Dense Layers.</li>
											<li>Classification with a convolutional architecture.</li>
											<li>Random Forest classifier.</li>
										</ul>
									These were all tested on a full dataset and datasets which were heavily truncated with 
									a principal component analysis.
									All in all, these three models had similar accuracy of somewhere around 99%. The convolutional architecture achieved 
									the best accuracy while the Random Forest on truncated data achieved the best overall performance with high accuracy and very low computational cost. 
									The problem with the convolutional network is that PCA can not be utilised here as it transforms 2D into 1D data. This makes the 
									classification a lot more complex than necessary. However, as this is computationally most 
									complex, the accuracy is also the best.
									</p>
									<p>
										<ul class="actions">
										<li><a href="PDF/Ising_model_classification.pdf" class="button">Full documentation</a></li>
										</ul>

								</p>



								<hr />
								<h2>Key Impact Factors of Carbon Emissions</h2>
								<p>
									Climate change is one of the biggest challenge humanity has ever faced. 
									In recent years mitigation efforts all around the globe have ramped up. 
									These efforts, however, have not been as successful as we hoped. 
									This research combines the Kaya identity, used to decompose carbon emissions into various 
									factors, with panel data analysis to extract the key impact factors on the changes in 
									growth of carbon emissions. The ultimate goal was to develope some policy recommendations to be able to tackle the climate crisis 
									more efficiently.
									Data from 120 different countries and territories from 1961 until 2015 provided by the World Bank
									was used. <br>
									What is the Kaya identity?
									To analyse and understand the changes in carbon emissions, the most popular tool used in literature is 
									the Kaya identity. This tool has been used in most major analysis on carbon emissions. 
									It is a very simple equation that takes 4 factors into account and links them to carbon emissions:
									$$F = P \times \frac{G}{P} \times \frac{E}{G} \times \frac{F}{E}$$
									In this equation,
									<ul>
										<li> \(F\) represents \(CO_2\) emissions from human sources,</li>
										<li> \(P\) represents population,</li>
										<li> \(G\) represents GDP,</li>
										<li> \(E\) represents energy consumption.</li>
									</ul>
									This is a simple yet beautiful way to link carbon emissions to the three factors GDP per capita, energy consumption per economic output and energy intensity.
									I have extended this Kaya identity and subsequently applied panel data analysis. In total the equation looks like this:

									$$  \Delta F =a+ b\times \Delta P_{(i,t)}+ c\times \Delta (\frac{G}{P})_{(i,t)}+d\times \Delta (\frac{E}{G})_{(i,t)}+ f\times \Delta ( \frac{F}{E})_{(i,t)}+ g\times \Delta Ex_{(i,t)} +h \times Gi_{(i,t)} + e_{(i,t)}$$
									This formula might look very complicated, but breaking it down into its components, this becomes very clear.
</p>
<p>
									<div class="table-wrapper">
										<table class="alt">
											<thead>
												<tr>
													<th>Name</th>
													<th>Description</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>\(\Delta P_{(i,t)}\)</td>
													<td>Population growth in %.</td>
												</tr>
												<tr>
													<td>\(\Delta (\frac{G}{P})_{(i,t)}\)</td>
													<td>Change in GDP per capita in %.</td>
												</tr>
												<tr>
													<td>\(\Delta Ex_{(i,t)}\)</td>
													<td> Change in exports in %.</td>
												</tr>
												<tr>
													<td>\(\Delta (\frac{E}{G})_{(i,t)}\)</td>
													<td>Change in energy efficiency in %.</td>
												</tr>
												<tr>
													<td>\(\Delta ( \frac{F}{E})_{(i,t)}\)</td>
													<td>Change in carbon intensity in %.</td>
												</tr>
												<tr>
													<td>\(Gi_{(i,t)}\)</td>
													<td>Gini index measuring income inequality.</td>
												</tr>
												<tr>
													<td>\(e_{(i,t)}\)</td>
													<td>Error term.</td>
												</tr>
												<tr>
													<td>\(a - h\)</td>
													<td>Coefficients for the fit.</td>
												</tr>
											</tbody>
										</table>
									</div>
									<span class="image left"><img src="images/Interaction_graph.png" alt="" /></span>
									This means that in this research, the change in carbon emissions has been linked to all the factors described in the table above.

									I decided to go with panel data analysis to extract as much information from the data as possible. 
									This type of analysis also takes the time sequence of the data into account.
									The results show that changes in technological improvements have the strongest global effects, 
									followed by changes in population growth and affluence. Finally, the impact of 
									income inequality on the change of carbon emissions is only relevant in high income nations. 
									This can be very nicely seen in the figure on the left. There is no significance for income inequality for less wealthy nations,
									while developed and rich nations show a correlation.
									<br>
									These results have some important policy implications to effectively tackle the problem of carbon emissions: <br>
									Obviously very few nations are willing to decrease their economic output to reduce carbon emission. 
									Countries are only willing to participate in carbon mitigation efforts, if the total welfare of a nation increases. 
									Currently the welfare of a nation is highly dependent on economic growth. Consequently, decreases in the growth of carbon emissions must be driven 
									from different areas. To reduce the growth of carbon emissions, without affecting economic productivity, 
									first and foremost the problem of population growth has to be tackled especially in developing countries. 
									Technological advancements and economic growth are less important, but certainly still a factor. The Kyoto protocol and the Paris agreement both 
									already have provisions of developed nations (Annex 1 countries) pledging to help developing nations (Annex 2 countries) to improve technological advancements. 
									The focus here should be on improving energy efficiency. This mean that more funds should be put into decreasing the amount of carbon that is produced 
									for every unit of economic output. This can be done by decreasing the amount of energy machinery in manufacturing need (increase efficiency). 
									Investments into renewable energy are less important in changing the direction of growth in carbon emissions. 
									Finally, for low income countries, counter intuitively it is important to increase the average income of the population. 
									Before all the basic needs are met, no focus on pro environmental actions can be taken. Once poverty is eradicated, income inequality 
									can be tackled to decrease the growth of carbon emissions. However, at least in the context of climate change, improving income inequality will not 
									help in the mitigation of carbon emissions.
									In high income countries, population growth is already almost constant and due to the fact that countries are not willing to 
									decrease their economic output, technological advancements and social factors would have to be improved. 
									Conveniently, the key impact factor on change in carbon emission in high income countries is technology. 
									Finally, tackling income inequality in very rich nations might also help to reduce the growth carbon emission.
								</p>


								<p>
									<ul class="actions">
									<li><a href="PDF/POL__Paper.pdf" class="button">Full paper</a></li>
									</ul>

							</p>






							</section>

					</div>
				</div>


			<!-- Footer -->
			<footer id="footer">
				<ul class="icons">
					<li><a href="https://www.linkedin.com/in/luca-schinnerl" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
					<li><a href="https://github.com/LucaSchinnerl" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
					<li><a href="mailto:schinnerl.luca@gmail.com" class="icon solid alt fa-envelope"><span class="label">Email</span></a></li>
				</ul>
				<ul class="copyright">
					<li>&copy; Luca Schinnerl. All rights reserved.</li>
				</ul>
				<ul class="copyright">
					<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
				</ul>
			</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
			<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
				MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
			</script>


	</body>
</html>